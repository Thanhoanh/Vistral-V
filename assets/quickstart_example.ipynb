{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Vistral-V: A Vietnamese Large Vision-Language Model\n\nThis project demonstrates visual instruction tuning applied to the Vistral large language model (LLM). This enables the model to understand and respond to prompts that combine both visual and textual information in Vietnamese.\n\n**Key features:**\n\n* **Multimodal understanding:** Process images and Vietnamese text together.\n* **Instruction following:** Respond to complex instructions that involve visual input.\n* **Vietnamese language specialization:** Fine-tuned for accuracy and fluency in Vietnamese.\n\n**Resources:**\n\n* **GitHub repository:** [Vistral-V](https://github.com/hllj/Vistral-V/tree/main?tab=readme-ov-file) (Codebase and details)\n* **HuggingFace model:** [Vistral](https://huggingface.co/Vi-VLM/Vista) (Try it out!)","metadata":{}},{"cell_type":"markdown","source":"## Login HuggingFace","metadata":{}},{"cell_type":"code","source":"# Add Read HuggingFace Token to download weight models\nfrom kaggle_secrets import UserSecretsClient\nfrom huggingface_hub.hf_api import HfFolder\n\n# Get HF Token, please add HF read token to Kaggle secret, or token = \"hf...\"\ntoken = UserSecretsClient().get_secret(\"HF_TOKEN\") \n\nHfFolder.save_token(token)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T18:11:25.802104Z","iopub.execute_input":"2024-06-29T18:11:25.802473Z","iopub.status.idle":"2024-06-29T18:11:26.309749Z","shell.execute_reply.started":"2024-06-29T18:11:25.802432Z","shell.execute_reply":"2024-06-29T18:11:26.308763Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Environment Setup\n\n1. Git clone our repo\n2. Install the customized package (which supports best for the model)\n3. Install other requirements from pip","metadata":{}},{"cell_type":"code","source":"# Clone the repository and install the packages of vistral-V\n!git clone https://github.com/hllj/Vistral-V.git\n%cd Vistral-V\n!pip install -e .\n\n# Install customized transformers\n!pip install transformers==4.41.2 -q\n\n# Install stuff\n!pip install transformers==4.41.2 ipykernel -q\n!pip install ipywidgets -q\n!jupyter nbextension enable --py widgetsnbextension","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","scrolled":true,"execution":{"iopub.status.busy":"2024-06-29T18:11:26.310798Z","iopub.execute_input":"2024-06-29T18:11:26.311068Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"fatal: destination path 'Vistral-V' already exists and is not an empty directory.\n/kaggle/working/Vistral-V\nObtaining file:///kaggle/working/Vistral-V\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch==2.1.2 in /opt/conda/lib/python3.10/site-packages (from llava==1.2.2.post1) (2.1.2)\nRequirement already satisfied: torchvision==0.16.2 in /opt/conda/lib/python3.10/site-packages (from llava==1.2.2.post1) (0.16.2)\nCollecting transformers==4.37.2 (from llava==1.2.2.post1)\n  Using cached transformers-4.37.2-py3-none-any.whl.metadata (129 kB)\nCollecting tokenizers==0.15.1 (from llava==1.2.2.post1)\n  Using cached tokenizers-0.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: sentencepiece==0.1.99 in /opt/conda/lib/python3.10/site-packages (from llava==1.2.2.post1) (0.1.99)\nRequirement already satisfied: shortuuid in /opt/conda/lib/python3.10/site-packages (from llava==1.2.2.post1) (1.0.13)\nRequirement already satisfied: accelerate==0.21.0 in /opt/conda/lib/python3.10/site-packages (from llava==1.2.2.post1) (0.21.0)\nRequirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (from llava==1.2.2.post1) (0.11.1)\nRequirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (from llava==1.2.2.post1) (0.43.1)\nRequirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (from llava==1.2.2.post1) (2.5.3)\nRequirement already satisfied: markdown2[all] in /opt/conda/lib/python3.10/site-packages (from llava==1.2.2.post1) (2.4.13)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from llava==1.2.2.post1) (1.26.4)\nRequirement already satisfied: scikit-learn==1.2.2 in /opt/conda/lib/python3.10/site-packages (from llava==1.2.2.post1) (1.2.2)\nRequirement already satisfied: gradio==4.16.0 in /opt/conda/lib/python3.10/site-packages (from llava==1.2.2.post1) (4.16.0)\nRequirement already satisfied: gradio-client==0.8.1 in /opt/conda/lib/python3.10/site-packages (from llava==1.2.2.post1) (0.8.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from llava==1.2.2.post1) (2.32.3)\nRequirement already satisfied: httpx==0.24.0 in /opt/conda/lib/python3.10/site-packages (from llava==1.2.2.post1) (0.24.0)\nRequirement already satisfied: uvicorn in /opt/conda/lib/python3.10/site-packages (from llava==1.2.2.post1) (0.25.0)\nRequirement already satisfied: fastapi in /opt/conda/lib/python3.10/site-packages (from llava==1.2.2.post1) (0.108.0)\nRequirement already satisfied: einops==0.6.1 in /opt/conda/lib/python3.10/site-packages (from llava==1.2.2.post1) (0.6.1)\nRequirement already satisfied: einops-exts==0.0.4 in /opt/conda/lib/python3.10/site-packages (from llava==1.2.2.post1) (0.0.4)\nRequirement already satisfied: timm==0.6.13 in /opt/conda/lib/python3.10/site-packages (from llava==1.2.2.post1) (0.6.13)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->llava==1.2.2.post1) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->llava==1.2.2.post1) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->llava==1.2.2.post1) (6.0.1)\nRequirement already satisfied: aiofiles<24.0,>=22.0 in /opt/conda/lib/python3.10/site-packages (from gradio==4.16.0->llava==1.2.2.post1) (22.1.0)\nRequirement already satisfied: altair<6.0,>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from gradio==4.16.0->llava==1.2.2.post1) (5.3.0)\nRequirement already satisfied: ffmpy in /opt/conda/lib/python3.10/site-packages (from gradio==4.16.0->llava==1.2.2.post1) (0.3.2)\nRequirement already satisfied: huggingface-hub>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from gradio==4.16.0->llava==1.2.2.post1) (0.23.2)\nRequirement already satisfied: importlib-resources<7.0,>=1.3 in /opt/conda/lib/python3.10/site-packages (from gradio==4.16.0->llava==1.2.2.post1) (6.1.1)\nRequirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.10/site-packages (from gradio==4.16.0->llava==1.2.2.post1) (3.1.2)\nRequirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio==4.16.0->llava==1.2.2.post1) (2.1.3)\nRequirement already satisfied: matplotlib~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio==4.16.0->llava==1.2.2.post1) (3.7.5)\nRequirement already satisfied: orjson~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio==4.16.0->llava==1.2.2.post1) (3.9.10)\nRequirement already satisfied: pandas<3.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio==4.16.0->llava==1.2.2.post1) (2.2.1)\nRequirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio==4.16.0->llava==1.2.2.post1) (9.5.0)\nRequirement already satisfied: pydub in /opt/conda/lib/python3.10/site-packages (from gradio==4.16.0->llava==1.2.2.post1) (0.25.1)\nRequirement already satisfied: python-multipart in /opt/conda/lib/python3.10/site-packages (from gradio==4.16.0->llava==1.2.2.post1) (0.0.9)\nRequirement already satisfied: ruff>=0.1.7 in /opt/conda/lib/python3.10/site-packages (from gradio==4.16.0->llava==1.2.2.post1) (0.5.0)\nRequirement already satisfied: semantic-version~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio==4.16.0->llava==1.2.2.post1) (2.10.0)\nRequirement already satisfied: tomlkit==0.12.0 in /opt/conda/lib/python3.10/site-packages (from gradio==4.16.0->llava==1.2.2.post1) (0.12.0)\nRequirement already satisfied: typer<1.0,>=0.9 in /opt/conda/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio==4.16.0->llava==1.2.2.post1) (0.9.0)\nRequirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.10/site-packages (from gradio==4.16.0->llava==1.2.2.post1) (4.9.0)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from gradio-client==0.8.1->llava==1.2.2.post1) (2024.3.1)\nRequirement already satisfied: websockets<12.0,>=10.0 in /opt/conda/lib/python3.10/site-packages (from gradio-client==0.8.1->llava==1.2.2.post1) (11.0.3)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx==0.24.0->llava==1.2.2.post1) (2024.2.2)\nRequirement already satisfied: httpcore<0.18.0,>=0.15.0 in /opt/conda/lib/python3.10/site-packages (from httpx==0.24.0->llava==1.2.2.post1) (0.17.3)\nRequirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx==0.24.0->llava==1.2.2.post1) (3.6)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx==0.24.0->llava==1.2.2.post1) (1.3.0)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.2.2->llava==1.2.2.post1) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.2.2->llava==1.2.2.post1) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.2.2->llava==1.2.2.post1) (3.2.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2->llava==1.2.2.post1) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2->llava==1.2.2.post1) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2->llava==1.2.2.post1) (3.2.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.37.2->llava==1.2.2.post1) (2023.12.25)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.37.2->llava==1.2.2.post1) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.37.2->llava==1.2.2.post1) (4.66.4)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic->llava==1.2.2.post1) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic->llava==1.2.2.post1) (2.14.6)\nRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn->llava==1.2.2.post1) (8.1.7)\nRequirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn->llava==1.2.2.post1) (0.14.0)\nRequirement already satisfied: starlette<0.33.0,>=0.29.0 in /opt/conda/lib/python3.10/site-packages (from fastapi->llava==1.2.2.post1) (0.32.0.post1)\nRequirement already satisfied: pygments>=2.7.3 in /opt/conda/lib/python3.10/site-packages (from markdown2[all]->llava==1.2.2.post1) (2.17.2)\nRequirement already satisfied: wavedrom in /opt/conda/lib/python3.10/site-packages (from markdown2[all]->llava==1.2.2.post1) (2.0.3.post3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->llava==1.2.2.post1) (3.3.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->llava==1.2.2.post1) (1.26.18)\nRequirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio==4.16.0->llava==1.2.2.post1) (4.20.0)\nRequirement already satisfied: toolz in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio==4.16.0->llava==1.2.2.post1) (0.12.1)\nRequirement already satisfied: anyio<5.0,>=3.0 in /opt/conda/lib/python3.10/site-packages (from httpcore<0.18.0,>=0.15.0->httpx==0.24.0->llava==1.2.2.post1) (4.2.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==4.16.0->llava==1.2.2.post1) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==4.16.0->llava==1.2.2.post1) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==4.16.0->llava==1.2.2.post1) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==4.16.0->llava==1.2.2.post1) (1.4.5)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==4.16.0->llava==1.2.2.post1) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==4.16.0->llava==1.2.2.post1) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio==4.16.0->llava==1.2.2.post1) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio==4.16.0->llava==1.2.2.post1) (2023.4)\nRequirement already satisfied: colorama<0.5.0,>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio==4.16.0->llava==1.2.2.post1) (0.4.6)\nRequirement already satisfied: shellingham<2.0.0,>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio==4.16.0->llava==1.2.2.post1) (1.5.4)\nRequirement already satisfied: rich<14.0.0,>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio==4.16.0->llava==1.2.2.post1) (13.7.0)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.1.2->llava==1.2.2.post1) (1.3.0)\nRequirement already satisfied: svgwrite in /opt/conda/lib/python3.10/site-packages (from wavedrom->markdown2[all]->llava==1.2.2.post1) (1.4.3)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from wavedrom->markdown2[all]->llava==1.2.2.post1) (1.16.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->httpcore<0.18.0,>=0.15.0->httpx==0.24.0->llava==1.2.2.post1) (1.2.0)\nRequirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.16.0->llava==1.2.2.post1) (23.2.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.16.0->llava==1.2.2.post1) (2023.12.1)\nRequirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.16.0->llava==1.2.2.post1) (0.32.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.16.0->llava==1.2.2.post1) (0.16.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio==4.16.0->llava==1.2.2.post1) (3.0.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio==4.16.0->llava==1.2.2.post1) (0.1.2)\nUsing cached tokenizers-0.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\nUsing cached transformers-4.37.2-py3-none-any.whl (8.4 MB)\nBuilding wheels for collected packages: llava\n  Building editable for llava (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for llava: filename=llava-1.2.2.post1-0.editable-py3-none-any.whl size=10319 sha256=caf859bfe410954379eb1b1b8191bbb25aaf956f35b498831a59722f354a07df\n  Stored in directory: /tmp/pip-ephem-wheel-cache-tg4d7f6u/wheels/03/f8/b5/2c1390454b11460afdf3d56cfebcffc5e58de711b43b26ab9c\nSuccessfully built llava\nInstalling collected packages: tokenizers, transformers, llava\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.19.1\n    Uninstalling tokenizers-0.19.1:\n      Successfully uninstalled tokenizers-0.19.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.41.2\n    Uninstalling transformers-4.41.2:\n      Successfully uninstalled transformers-4.41.2\n  Attempting uninstall: llava\n    Found existing installation: llava 1.2.2.post1\n    Uninstalling llava-1.2.2.post1:\n      Successfully uninstalled llava-1.2.2.post1\nSuccessfully installed llava-1.2.2.post1 tokenizers-0.15.1 transformers-4.37.2\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Getting Started\n\nThis Kaggle example demonstrates how to use [Vistral Vision](https://huggingface.co/Vi-VLM/Vista), an AI model that can understand and discuss images in Vietnamese. \n\n**What you'll see:**\n* **Sets up the Vistral Vision model:** We'll load the model and get it ready to process images and text.\n* **Interactive conversations:**  Ask questions about images, and Vistral Vision will respond in Vietnamese.\n\n**Try it yourself!** Follow the steps below to run the code and start a conversation with Vistral Vision.","metadata":{}},{"cell_type":"markdown","source":"### Setting configs","metadata":{}},{"cell_type":"code","source":"model_path = \"Vi-VLM/llava-vistral-7b-lora\"\nmodel_base = \"Viet-Mistral/Vistral-7B-Chat\"\nconv_mode = \"vistral\"\n\nimage_file = \"./images/example.jpeg\"\n\ntemperature = 0.2\nmax_new_tokens = 512\nload_8bit = False\nload_4bit = False\n\ndebug = False\ndevice = \"cuda:0\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Import Libraries and Load the Model","metadata":{}},{"cell_type":"markdown","source":"To ensure optimal model performance and compatibility in diverse environments, we enable `disable_torch_init`. This helps prevent conflicts and improves resource management when deploying the model.","metadata":{}},{"cell_type":"code","source":"from llava.utils import disable_torch_init\n\ndisable_torch_init()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from llava.model.builder import load_pretrained_model\nfrom llava.mm_utils import get_model_name_from_path\n\n# Get the model name\nmodel_name = get_model_name_from_path(model_path)\n\n# Load model, tokenizer and processor stuff\ntokenizer, model, image_processor, context_len = load_pretrained_model(\n    model_path=model_path,\n    model_base=model_base,\n    model_name=model_name,\n    load_8bit=load_8bit,\n    load_4bit=load_4bit,\n    device=device\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load image using","metadata":{}},{"cell_type":"code","source":"import requests\nfrom PIL import Image\nfrom io import BytesIO\n\ndef load_image(image_file):\n    if image_file.startswith('http://') or image_file.startswith('https://'):\n        response = requests.get(image_file)\n        image = Image.open(BytesIO(response.content)).convert('RGB')\n    else:\n        image = Image.open(image_file).convert('RGB')\n    return image\n\nimage = load_image(image_file)\n\nprint(\"This is the image using\")\ndisplay(image)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Configuring the Conversation Template","metadata":{}},{"cell_type":"code","source":"from llava.conversation import conv_templates\n\nif \"llama-2\" in model_name.lower():\n    conv_mode = \"llava_llama_2\"\nelif \"mistral\" in model_name.lower():\n    conv_mode = \"mistral_instruct\"\nelif \"vistral\" in model_name.lower():\n    conv_mode = \"vistral\"\nelif \"v1.6-34b\" in model_name.lower():\n    conv_mode = \"chatml_direct\"\nelif \"v1\" in model_name.lower():\n    conv_mode = \"llava_v1\"\nelif \"mpt\" in model_name.lower():\n    conv_mode = \"mpt\"\nelse:\n    conv_mode = \"llava_v0\"\n\nconv = conv_templates[conv_mode].copy()\nif \"mpt\" in model_name.lower():\n    roles = ('user', 'assistant')\nelse:\n    roles = conv.roles","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's chat !!","metadata":{}},{"cell_type":"markdown","source":"### Set Up Your Workspace\nStart by importing the required libraries and loading the image you want to analyze.\n","metadata":{}},{"cell_type":"code","source":"import torch\n\nfrom llava.conversation import SeparatorStyle\nfrom llava.mm_utils import process_images, tokenizer_image_token\nfrom llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n\nfrom transformers import TextStreamer\nfrom IPython.display import display\n\n# Get the image size and dÃ­play it\nimage_size = image.size\n\n# Similar operation in model_worker.py\nimage_tensor = process_images([image], image_processor, model.config)\nif type(image_tensor) is list:\n    image_tensor = [image.to(model.device, dtype=torch.float16) for image in image_tensor]\nelse:\n    image_tensor = image_tensor.to(model.device, dtype=torch.float16)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Chat !","metadata":{}},{"cell_type":"code","source":"while True:\n    try:\n        inp = input(f\"{roles[0]}: \")\n    except EOFError:\n        inp = \"\"\n    if not inp:\n        print(\"exit...\")\n        break\n\n    print(f\"{roles[1]}: \", end=\"\")\n\n    if image is not None:\n        # first message\n        if model.config.mm_use_im_start_end:\n            inp = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + '\\n' + inp\n        else:\n            inp = DEFAULT_IMAGE_TOKEN + '\\n' + inp\n        image = None\n\n    conv.append_message(conv.roles[0], inp)\n    conv.append_message(conv.roles[1], None)\n    prompt = conv.get_prompt()\n\n    input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).to(model.device)\n    stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n    keywords = [stop_str]\n    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n\n    with torch.inference_mode():\n        output_ids = model.generate(\n            input_ids,\n            images=image_tensor,\n            image_sizes=[image_size],\n            do_sample=True if temperature > 0 else False,\n            temperature=temperature,\n            max_new_tokens=max_new_tokens,\n            streamer=streamer,\n            use_cache=True)\n\n    outputs = tokenizer.decode(output_ids[0]).strip()\n    conv.messages[-1][-1] = outputs\n\n    if debug:\n        print(\"\\n\", {\"prompt\": prompt, \"outputs\": outputs}, \"\\n\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}